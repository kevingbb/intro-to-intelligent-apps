{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Embeddings, Memories and Vectors\n",
    "\n",
    "In this lab, we'll explore how we can bring our own data into the models used by Azure OpenAI.\n",
    "\n",
    "We'll start as usual by defining our Azure OpenAI service API key and endpoint details, specifying the model deployment we want to use and then we'll initiate a connection to the Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "DEPLOYMENT_ID = \"text-davinci-003\" # For example \"text-davinci-003\"\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = DEPLOYMENT_ID\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask the AI a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API\n",
    "r = llm(\"Tell me about the latest Ant Man movie. When was it released? What is it about?\")\n",
    "\n",
    "# Print the response\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the response?\n",
    "\n",
    "The AI thinks the latest \"Ant-Man\" movie was \"Ant-Man and the Wasp\" and it was released in July 2018. \n",
    "\n",
    "OpenAI models are trained on a large set of data, but that happened at a specific point in time depending on the model. So, many of the models have no information about events that took place in recent months or years.\n",
    "\n",
    "To help the AI out, we can provide additional information. This is the same process you would follow if you want the AI to work with your own company data. The AI won't know about information that you don't make publically available, so if you want the AI to work with that information, then you'll need to get that information into the model.\n",
    "\n",
    "The thing is, you can't actually do that. The models are pre-trained, so the only way to get more information in is to retrain the model, which is an expensive and time consuming process.\n",
    "\n",
    "However, there *are* ways to get the AI models to work with new data. The most popular of these methods is to use *embeddings*, which we'll explore in the next sections.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Your Own Data\n",
    "\n",
    "Langchain provides a number of useful tools, which include tools to simplify the process of working with external documents. Below, we'll use the `DirectoryLoader` which can read multiple files from a directory and the `UnstructuredMarkdownLoader` which can process files in Markdown format. We'll use these to process a bunch of markdown formatted files that contain details of movies that were released in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "data_dir = \"data/movies\"\n",
    "\n",
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `documents` object which contains all of the information from our documents about movies.\n",
    "\n",
    "Let's use the `question_answering` chain to query our AI again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The model now knows about the latest Ant-Man movie.\n",
    "\n",
    "However, there's something lurking! Let's take a look at what happened behind the scenes.\n",
    "\n",
    "We'll do two things here. First we'll add the `verbose=True` parameter to the chain, and we'll wrap the chain execution in a callback, which will allow us to capture the number of tokens consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for callbacks\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm, verbose=True)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    chain.run(input_documents=documents, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That request used around 2,900 tokens! That's a lot of tokens. Plus, with the verbose option enabled, you can see that a prompt was constructed which included all of the information from our documents in the prompt, which is why it used so many tokens.\n",
    "\n",
    "As we've discussed previously, AI models have a maximum number of tokens you can use. These are relatively small documents that we're working with here and there's only 20 of them, so clearly this is not going to scale when we want to work with larger documents and more of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "The solution to working with large amounts of external information is to use *Vectors*. In simple terms, vectors allow human readable information to be converted into a numeric format that allows computers to understand the meaning as well. We can convert data into vectors and store that vector information in a database. We can then run queries by converting our human language query into a vector and then attempting to match that vector with vectors in the database. If the vector that represents your query is similar to vectors in the database, then it's likely to be a good response to the query.\n",
    "\n",
    "To prevent overloading a prompt with a large number of tokens, we can perform a vector search first to narrow down to a set of interesting results, and then use that smaller subset of information as part of a prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating embeddings and ... usually looks something like this\n",
    "\n",
    "1. Use an embeddings model to vectorise documents.\n",
    "2. Save the vectors to a vector database\n",
    "3. Use an embeddings model to vectorise a query you want to perform\n",
    "4. Search the vector database using the vectorised query to find matches\n",
    "5. Use the search results to pass to the AI and \n",
    "\n",
    "AI Orchestration tools aim to simplify this process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `text-embedding-ada-002`. If your deployment of this model has a different name, replace the text below as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment = EMBEDDING_MODEL,\n",
    "    chunk_size = 1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `chunk_size = 1` parameter is used to workaround a temporary limitation in the Azure OpenAI API which only allows one embedding to be processed at a time with each call to the API.\n",
    "\n",
    "Now that we've initialised a model to create embeddings, let's go ahead and embed some documents.\n",
    "\n",
    "AI Orchestrators like Langchain and Semantic Kernel can help simplify the process of embedding, vectorization and search. In the code below, we use Langchain's document loader as we did previously to load and process our Markdown formatted documents. We also use a `VectorstoreIndexCreator` which you can see only requires a couple of parameters - the embedding model that we want to use and the source data (`loader`) to use. However, that simple code hides an awful lot of complexity.\n",
    "\n",
    "Behind the scenes, the `VectorstoreIndexCreator` does several things.\n",
    "\n",
    "- Documents are split into chunks. This is done to ensure that any large documents don't use more tokens than the models allow.\n",
    "- Embeddings (vectors) are created for each document.\n",
    "- Documents and embeddings are placed in a vector store database.\n",
    "- Create a `retriever` that can be used for querying.\n",
    "\n",
    "You can implement each of these steps yourself using Langchain, which will give you more control over the process. However, using the `VectorstoreIndexCreator` provides a quick solution for this walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=OpenAIEmbeddings(chunk_size=1)\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Depending on your configuration, you might hit a `RateLimitError` when running the above. However, you will notice that Langchain detects that you were rate limited and automatically retries the request after a few seconds. This is another of the advantages of using an AI Orchestrator!\n",
    "\n",
    "Now, to run a query against our data, we just need to specify the prompt and then call the index we've created above and pass in the model (`llm`) we want to use and the question we want to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "index.query(llm=llm, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks the same as the result we had previously, as the AI has been able to return correct details about the very latest Ant-Man movie that was released in 2023. So, what's the difference?\n",
    "\n",
    "In this case, the query that you pass in is vectorised and then the vector database that was created behind the scenes is searched. Any matches found in the database are then used when sending the complete prompt to the AI, rather than using all of the documents are we did before.\n",
    "\n",
    "Like we did last time, let's use a callback so we can see how many tokens were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    index.query(llm=llm, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact number of tokens used may vary, but it should be clear that this query will have used far fewer tokens than our original query, typically around 2,000 fewer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
