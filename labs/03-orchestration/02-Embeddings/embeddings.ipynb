{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Embeddings and Vectors\n",
    "\n",
    "In this lab, we'll explore how we can bring our own data into the models used by Azure OpenAI.\n",
    "\n",
    "We'll start as usual by defining our Azure OpenAI service API key and endpoint details, specifying the model deployment we want to use and then we'll initiate a connection to the Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "DEPLOYMENT_ID = \"text-davinci-003\" # For example \"text-davinci-003\"\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = DEPLOYMENT_ID\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask the AI a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ant-Man and the Wasp is the latest movie in the Marvel Cinematic Universe and the sequel to 2015's Ant-Man. The film was released in the United States on July 6, 2018. The movie follows Scott Lang (Paul Rudd) as he balances his home life as a father with his responsibilities as Ant-Man. He is enlisted by Dr. Hank Pym (Michael Douglas) and his daughter Hope van Dyne (Evangeline Lilly) to help them with a secret mission. They must find Hope's mother, Janet van Dyne (Michelle Pfeiffer), who is lost in the Quantum Realm. In the process, they must deal with a new villain, Ghost (Hannah John-Kamen).\n"
     ]
    }
   ],
   "source": [
    "# Call the API\n",
    "r = llm(\"Tell me about the latest Ant-Man movie. When was it released? What is it about?\")\n",
    "\n",
    "# Print the response\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the response?\n",
    "\n",
    "The AI thinks the latest \"Ant-Man\" movie was \"Ant-Man and the Wasp\" and it was released in July 2018. \n",
    "\n",
    "OpenAI models are trained on a large set of data, but that happened at a specific point in time depending on the model. So, many of the models have no information about events that took place in recent months or years.\n",
    "\n",
    "To help the AI out, we can provide additional information. This is the same process you would follow if you want the AI to work with your own company data. The AI won't know about information that you don't make publically available, so if you want the AI to work with that information, then you'll need to get that information into the model.\n",
    "\n",
    "The thing is, you can't actually do that. The models are pre-trained, so the only way to get more information in is to retrain the model, which is an expensive and time consuming process.\n",
    "\n",
    "However, there *are* ways to get the AI models to work with new data. The most popular of these methods is to use *embeddings*, which we'll explore in the next sections.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Your Own Data\n",
    "\n",
    "Langchain provides a number of useful tools, which include tools to simplify the process of working with external documents. Below, we'll use the `DirectoryLoader` which can read multiple files from a directory and the `UnstructuredMarkdownLoader` which can process files in Markdown format. We'll use these to process a bunch of markdown formatted files that contain details of movies that were released in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "data_dir = \"data/movies\"\n",
    "\n",
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `documents` object which contains all of the information from our documents about movies.\n",
    "\n",
    "Let's use the `question_answering` chain to query our AI again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The latest Ant Man movie is called Ant Man and the Wasp: Quantumania and it was released on 2023-02-15. It is about Super-Hero partners Scott Lang and Hope van Dyne, along with with Hope's parents Janet van Dyne and Hank Pym, and Scott's daughter Cassie Lang, finding themselves exploring the Quantum Realm, interacting with strange new creatures and embarking on an adventure that will push them beyond the limits of what they thought possible.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question answering chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The model now knows about the latest Ant-Man movie.\n",
    "\n",
    "However, there's something lurking! Let's take a look at what happened behind the scenes.\n",
    "\n",
    "We'll do two things here. First we'll add the `verbose=True` parameter to the chain, and we'll wrap the chain execution in a callback, which will allow us to capture the number of tokens consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for callbacks\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm, verbose=True)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    chain.run(input_documents=documents, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That request used around 2,900 tokens! That's a lot of tokens. Plus, with the verbose option enabled, you can see that a prompt was constructed which included all of the information from our documents in the prompt, which is why it used so many tokens.\n",
    "\n",
    "As we've discussed previously, AI models have a maximum number of tokens you can use. These are relatively small documents that we're working with here and there's only 20 of them, so clearly this is not going to scale when we want to work with larger documents and more of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "The solution to working with large amounts of external information is to use *Vectors*. In simple terms, vectors allow human readable information to be converted into a numeric format that allows computers to understand the meaning as well. We can convert data into vectors and store that vector information in a database. We can then run queries by converting our human language query into a vector and then attempting to match that vector with vectors in the database. If the vector that represents your query is similar to vectors in the database, then it's likely to be a good response to the query.\n",
    "\n",
    "To prevent overloading a prompt with a large number of tokens, we can perform a vector search first to narrow down to a set of interesting results, and then use that smaller subset of information as part of a prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating embeddings and ... usually looks something like this\n",
    "\n",
    "1. Use an embeddings model to vectorise documents.\n",
    "2. Save the vectors to a vector database\n",
    "3. Use an embeddings model to vectorise a query you want to perform\n",
    "4. Search the vector database using the vectorised query to find matches\n",
    "5. Use the search results to pass to the AI and \n",
    "\n",
    "AI Orchestration tools aim to simplify this process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `text-embedding-ada-002`. If your deployment of this model has a different name, replace the text below as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment = EMBEDDING_MODEL,\n",
    "    chunk_size = 1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `chunk_size = 1` parameter is used to workaround a temporary limitation in the Azure OpenAI API which only allows one embedding to be processed at a time with each call to the API.\n",
    "\n",
    "Now that we've initialised a model to create embeddings, let's go ahead and embed some documents.\n",
    "\n",
    "As we did in the previous example, we'll use Langchain's builit in loaders to read the documents from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 152.52it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use a *splitter*. A splitter enables us to break up larger documents into chunks, so that we don't risk hitting the token limit when submitting our data to the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "document_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage is to convert the chunks of split documents into vectors which we do by passing the data through an embedding model. The resultant vectors are then stored in a vector database. In this example, we're using the **Qdrant** (pronounced 'quadrant') database. We initialise it using the `location=\":memory:\"` option, so that the database will be stored in memory rather than persisted to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 18.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    document_chunks,\n",
    "    embeddings_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"movies\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run our query again. However, we'll make one small change.\n",
    "\n",
    "You may be thinking that it's not surprising that the AI now knows about the latest Ant-Man movie, because we told it about the lastest Ant-Man movie! So, let's try and show that the AI is actually doing some work here.\n",
    "\n",
    "If you're not a fan of these movies, Ant-Man originates from Marvel comic books. And the collection of movies that originate from Marvel comic books are said to be part of the Marvel Cinematic Universe, sometimes referred to as the MCU. We haven't mentioned Marvel or MCU in the data we've provided, so if we ask the AI about the MCU, let's see if it can use it's models to figure out what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The latest MCU movie is Ant-Man and the Wasp: Quantumania. It was released on February 15, 2023. It follows the story of Scott Lang and Hope van Dyne, along with with Hope's parents Janet van Dyne and Hank Pym, and Scott's daughter Cassie Lang, who explore the Quantum Realm and interact with strange creatures. They must save the world from a new threat.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about the latest MCU movie. When was it released? What is it about?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Orchestrators like Langchain and Semantic Kernel can help simplify the process of embedding, vectorization and search. In the code below, we use Langchain's document loader as we did previously to load and process our Markdown formatted documents. We also use a `VectorstoreIndexCreator` which you can see only requires a couple of parameters - the embedding model that we want to use and the source data (`loader`) to use. However, that simple code hides an awful lot of complexity.\n",
    "\n",
    "Behind the scenes, the `VectorstoreIndexCreator` does several things.\n",
    "\n",
    "- Documents are split into chunks. This is done to ensure that any large documents don't use more tokens than the models allow.\n",
    "- Embeddings (vectors) are created for each document.\n",
    "- Documents and embeddings are placed in a vector store database.\n",
    "- Create a `retriever` that can be used for querying.\n",
    "\n",
    "You can implement each of these steps yourself using Langchain, which will give you more control over the process. However, using the `VectorstoreIndexCreator` provides a quick solution for this walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=OpenAIEmbeddings(chunk_size=1)\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Depending on your configuration, you might hit a `RateLimitError` when running the above. However, you will notice that Langchain detects that you were rate limited and automatically retries the request after a few seconds. This is another of the advantages of using an AI Orchestrator!\n",
    "\n",
    "Now, to run a query against our data, we just need to specify the prompt and then call the index we've created above and pass in the model (`llm`) we want to use and the question we want to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "index.query(llm=llm, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks the same as the result we had previously, as the AI has been able to return correct details about the very latest Ant-Man movie that was released in 2023. So, what's the difference?\n",
    "\n",
    "In this case, the query that you pass in is vectorised and then the vector database that was created behind the scenes is searched. Any matches found in the database are then used when sending the complete prompt to the AI, rather than using all of the documents are we did before.\n",
    "\n",
    "Like we did last time, let's use a callback so we can see how many tokens were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    index.query(llm=llm, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact number of tokens used may vary, but it should be clear that this query will have used far fewer tokens than our original query, typically around 2,000 fewer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
