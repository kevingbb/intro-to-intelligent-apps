{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Langchain with vector store\n",
    "\n",
    "In this lab, we will introduce [Langchain](https://python.langchain.com/docs/get_started/introduction), a framework for developing applications powered by language models and ask question on custom data using a vector store.\n",
    "\n",
    "Langchain supports Python and Javascript / Typescript. For this lab, we will use Python.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll use the `pip` tool to install the `langchain` Python package and `quadrant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai tiktoken qdrant-client --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the movies csv file the `AzureOpenAI` specific components from the `langchain` package.\n",
    "As with all the other labs, we'll need to provide our API key and endpoint details. We'll also provide the name (id) of the model deployment that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "API_KEY = \"<YOUR API KEY>\"\n",
    "RESOURCE_ENDPOINT = \"<YOUR AZURE OPENAI ENDPOINT>\" # For example https://<your azure open ai instance>.openai.azure.com/\n",
    "DEPLOYMENT_ID = \"<YOUR DEPLOYMENT ID>\" # For example \"text-davinci-003\"\n",
    "\n",
    "# Set this to `azure`\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = RESOURCE_ENDPOINT\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data from the csv file into a loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='../../extra/data/movies/movies.csv', source_column='original_title', encoding='utf-8', csv_args={'delimiter':',', 'fieldnames': ['id', 'original_language', 'original_title', 'popularity', 'release_date', 'vote_average', 'vote_count', 'genre', 'overview', 'revenue', 'runtime', 'tagline']})\n",
    "data = loader.load()\n",
    "data = data[1:]\n",
    "print('Loaded %s movies' % len(data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the OpenAI embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=DEPLOYMENT_ID,\n",
    "    model_name=\"gpt-35-turbo\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure Langchain to use Qdrant as vector store, embedd the loaded documents and store the embeddings in the vector store. Depending on the rate limiting this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    data,\n",
    "    embeddings,\n",
    "    path=\"../../extra/data/quadrant\", # location=\":memory:\",\n",
    "    collection_name=\"my_movies\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test the vector store and search for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the best 80s movie I should look?\"\n",
    "found_docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way would be to search for similar movies but with a more diverse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Which movies are about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using a QA chain to ask questions about the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(deployment_id=\"d1\")\n",
    "chain = RetrievalQA.from_chain_type(llm=openai, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\", return_source_documents=True)\n",
    "query = \"Do you have a column called popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])\n",
    "\n",
    "query = \"What is the movie with the highest popularity?\"\n",
    "response = chain({\"question\": query})\n",
    "print(response['result'])\n",
    "print(response['source_documents'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the vector database from a file and ask the same question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire in the Sky\n"
     ]
    }
   ],
   "source": [
    "#del vectorstore\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "import qdrant_client\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"../../extra/data/quadrant\", prefer_grpc=True)\n",
    "qdrant = Qdrant(client=client, collection_name=\"my_movies\", embeddings=embeddings)\n",
    "\n",
    "query = \"What movie was about flying to the moon?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a retriever to query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 8 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 4 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jetsons: The Movie\n"
     ]
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query = \"Which movies are about space travel?\"\n",
    "print(retriever.get_relevant_documents(query)[0].metadata['source'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
